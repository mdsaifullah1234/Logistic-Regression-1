{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of \n",
    "a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Linear regression predicts continuous outcomes, while logistic regression predicts binary outcomes. For example, linear regression could predict house prices based on square footage, while logistic regression could predict whether an email is spam or not based on features like subject line and sender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS =  The cost function used in logistic regression is the logistic loss or cross-entropy loss function. It is optimized using iterative optimization algorithms like gradient descent or Newton's method to minimize the difference between predicted probabilities and actual class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Regularization in logistic regression involves adding penalty terms to the cost function to discourage large coefficients. This helps prevent overfitting by penalizing overly complex models, making the model generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression \n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  The ROC (Receiver Operating Characteristic) curve is a graphical representation of the trade-off between true positive rate (sensitivity) and false positive rate (1 - specificity) for different threshold values. It is used to evaluate the performance of the logistic regression model by visualizing its ability to discriminate between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these \n",
    "techniques help improve the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Common techniques for feature selection in logistic regression include forward selection, backward elimination, L1 regularization (Lasso), and feature importance ranking. These techniques help improve the model's performance by selecting the most relevant features and reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing \n",
    "with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Imbalanced datasets in logistic regression occur when one class is significantly more prevalent than the other. Strategies for handling class imbalance include resampling techniques like oversampling the minority class or undersampling the majority class, using synthetic data generation methods like SMOTE, and adjusting class weights during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic \n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity \n",
    "among the independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  Common issues when implementing logistic regression include multicollinearity among independent variables, convergence problems during optimization, and class imbalance. Multicollinearity can be addressed by removing correlated predictors or using regularization techniques like Ridge Regression. Convergence problems can be mitigated by adjusting optimization parameters or using alternative optimization algorithms. Class imbalance can be handled using techniques mentioned earlier like resampling or adjusting class weights."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
